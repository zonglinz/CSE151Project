{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install numpy pandas scikit-learn scipy\n",
        "\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, MaxAbsScaler, normalize\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "DATA_PATH   = \"/content/data.csv\"\n",
        "OUT_DIR     = \"artifacts_model2_short\"\n",
        "EMBED_DIMS  = 192\n",
        "USE_L2      = True\n",
        "SEED        = 42\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(DATA_PATH).drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "\n",
        "LABEL_COL = None\n",
        "for c in df.columns:\n",
        "    if c.strip().lower() in {\"label\",\"labels\",\"target\",\"class\",\"classes\",\"category\",\"y\",\"outcome\",\"diagnosis\"}:\n",
        "        LABEL_COL = c; break\n",
        "if LABEL_COL is None and \"Class\" in df.columns:\n",
        "    LABEL_COL = \"Class\"\n",
        "if LABEL_COL is None:\n",
        "    n = len(df)\n",
        "    for c in df.columns:\n",
        "        nu = df[c].nunique(dropna=False)\n",
        "        if 2 <= nu <= min(50, max(2, int(0.2*n))):\n",
        "            LABEL_COL = c; break\n",
        "if LABEL_COL is None:\n",
        "    raise ValueError(\"Couldn't infer label column.\")\n",
        "\n",
        "y = df[LABEL_COL].astype(str).to_numpy()\n",
        "feat_cols = [c for c in df.columns if c != LABEL_COL]\n",
        "\n",
        "def is_num(s):\n",
        "    return pd.api.types.is_numeric_dtype(s)\n",
        "obj_cols      = [c for c in feat_cols if df[c].dtype == \"object\"]\n",
        "low_card_cols = [c for c in feat_cols if is_num(df[c]) and df[c].nunique() <= 5]\n",
        "cat_cols      = sorted(set(obj_cols + low_card_cols))\n",
        "num_cols      = [c for c in feat_cols if c not in cat_cols]\n",
        "\n",
        "\n",
        "Xtr_df, Xte_df, ytr, yte = train_test_split(df[feat_cols], y, test_size=0.20, stratify=y, random_state=SEED)\n",
        "\n",
        "\n",
        "def make_ohe_sparse():\n",
        "    try:    return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
        "    except: return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
        "\n",
        "cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", make_ohe_sparse())])\n",
        "num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"scale\", MaxAbsScaler())])\n",
        "\n",
        "trs = []\n",
        "if num_cols: trs.append((\"num\", num_pipe, num_cols))\n",
        "if cat_cols: trs.append((\"cat\", cat_pipe, cat_cols))\n",
        "pre_train = ColumnTransformer(trs, remainder=\"drop\", sparse_threshold=1.0)\n",
        "\n",
        "Xtr_pre = pre_train.fit_transform(Xtr_df)\n",
        "Xte_pre = pre_train.transform(Xte_df)\n",
        "n_features = Xtr_pre.shape[1]\n",
        "\n",
        "svd_dims = max(2, min(EMBED_DIMS, n_features - 1))\n",
        "svd = TruncatedSVD(n_components=svd_dims, random_state=SEED)\n",
        "\n",
        "Ztr = svd.fit_transform(Xtr_pre)\n",
        "Zte = svd.transform(Xte_pre)\n",
        "\n",
        "if USE_L2:\n",
        "    Ztr = normalize(Ztr); Zte = normalize(Zte)\n",
        "\n",
        "print(f\"[Info] Preprocessed features: {n_features} | SVD dims used: {svd_dims} | var≈{svd.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "def hungarian_map(clusters, labels):\n",
        "    clus = np.unique(clusters); labs = np.unique(labels)\n",
        "    M = np.zeros((len(clus), len(labs)), dtype=int)\n",
        "    ci = {c:i for i,c in enumerate(clus)}; li = {l:i for i,l in enumerate(labs)}\n",
        "    for c, y in zip(clusters, labels): M[ci[c], li[y]] += 1\n",
        "    r, c = linear_sum_assignment(-M)\n",
        "    mp = {clus[i]: labs[j] for i, j in zip(r, c)}\n",
        "    for c_id in clus:\n",
        "        if c_id not in mp: mp[c_id] = labs[np.argmax(M[ci[c_id]])]\n",
        "    return mp\n",
        "\n",
        "\n",
        "n_labels = len(np.unique(ytr))\n",
        "k_grid = sorted(set([n_labels, 2*n_labels, 3*n_labels, 4*n_labels]))\n",
        "\n",
        "Ztr_tr, Ztr_val, ytr_tr, ytr_val = train_test_split(Ztr, ytr, test_size=0.2, stratify=ytr, random_state=SEED)\n",
        "\n",
        "best = {\"k\": None, \"val_acc\": -1, \"model\": None, \"map\": None}\n",
        "for k in k_grid:\n",
        "    mbk = MiniBatchKMeans(n_clusters=k, random_state=SEED, n_init=15, batch_size=4096, max_iter=250).fit(Ztr_tr)\n",
        "    mapping = hungarian_map(mbk.labels_, ytr_tr)\n",
        "    yval_pred = np.array([mapping[c] for c in mbk.predict(Ztr_val)], dtype=object)\n",
        "    acc = accuracy_score(ytr_val, yval_pred)\n",
        "    if acc > best[\"val_acc\"]:\n",
        "        best.update({\"k\": k, \"val_acc\": acc, \"model\": mbk, \"map\": mapping})\n",
        "\n",
        "print(f\"[k-choice] k∈{k_grid} | picked k={best['k']} | val_acc={best['val_acc']:.4f}\")\n",
        "\n",
        "\n",
        "final = MiniBatchKMeans(n_clusters=best[\"k\"], random_state=SEED, n_init=20, batch_size=8192, max_iter=300).fit(Ztr)\n",
        "c_tr, c_te = final.labels_, final.predict(Zte)\n",
        "cl2lb = hungarian_map(c_tr, ytr)\n",
        "fallback = Counter(ytr).most_common(1)[0][0]\n",
        "ytr_pred = np.array([cl2lb.get(c, fallback) for c in c_tr], dtype=object)\n",
        "yte_pred = np.array([cl2lb.get(c, fallback) for c in c_te], dtype=object)\n",
        "\n",
        "def report(name, y_true, y_pred):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1m = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    print(f\"{name} Accuracy: {acc:.4f} | Macro-F1: {f1m:.4f}\")\n",
        "    print(classification_report(y_true, y_pred, zero_division=0))\n",
        "    return acc, f1m\n",
        "\n",
        "print(\"\\n=== MODEL 2 (SHORT+FIXED) RESULTS ===\")\n",
        "acc_tr, f1m_tr = report(\"TRAIN\", ytr, ytr_pred)\n",
        "acc_te, f1m_te = report(\"TEST \", yte, yte_pred)\n",
        "print(f\"Train Error: {1-acc_tr:.4f} | Test Error: {1-acc_te:.4f} | Gap: {(1-acc_te)-(1-acc_tr):.4f}\")\n",
        "\n",
        "labels = sorted(np.unique(np.concatenate([ytr, yte]).astype(str)))\n",
        "cm = confusion_matrix(yte, yte_pred, labels=labels)\n",
        "rows = []\n",
        "for i, lab in enumerate(labels):\n",
        "    TP = int(cm[i, i]); FN = int(cm[i, :].sum() - TP)\n",
        "    FP = int(cm[:, i].sum() - TP); TN = int(cm.sum() - TP - FP - FN)\n",
        "    rows.append({\"class\": lab, \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN})\n",
        "fpfn_test = pd.DataFrame(rows).set_index(\"class\")\n",
        "pred_df = pd.DataFrame({\"index\": Xte_df.index, \"y_true\": yte, \"cluster\": c_te, \"y_pred\": yte_pred, \"correct\": (yte_pred==yte)})\n",
        "\n",
        "fpfn_test.to_csv(os.path.join(OUT_DIR, \"fpfn_test.csv\"))\n",
        "pred_df.to_csv(os.path.join(OUT_DIR, \"predictions_test.csv\"), index=False)\n",
        "\n",
        "summary = {\n",
        "    \"embed\": {\"type\": \"TruncatedSVD\", \"dims\": int(svd_dims), \"l2\": bool(USE_L2),\n",
        "              \"preprocessed_features\": int(n_features), \"variance_est\": float(svd.explained_variance_ratio_.sum())},\n",
        "    \"k\": int(best[\"k\"]), \"val_acc\": float(best[\"val_acc\"]),\n",
        "    \"train\": {\"accuracy\": float(acc_tr), \"macro_f1\": float(f1m_tr), \"error\": float(1-acc_tr)},\n",
        "    \"test\":  {\"accuracy\": float(acc_te), \"macro_f1\": float(f1m_te), \"error\": float(1-acc_te),\n",
        "              \"gap\": float((1-acc_te)-(1-acc_tr))}\n",
        "}\n",
        "with open(os.path.join(OUT_DIR, \"results_summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"\\n[SUMMARY]\\n\", json.dumps(summary, indent=2))\n",
        "print(f\"\\nArtifacts in: {OUT_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2Bt8SL4gt_B",
        "outputId": "bb8d7ba4-c45f-49f4-c2a4-33789de2b10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Preprocessed features: 68 | SVD dims used: 67 | var≈1.0000\n",
            "[k-choice] k∈[9, 18, 27, 36] | picked k=36 | val_acc=0.7891\n",
            "\n",
            "=== MODEL 2 (SHORT+FIXED) RESULTS ===\n",
            "TRAIN Accuracy: 0.8306 | Macro-F1: 0.7239\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.89      0.87      1203\n",
            "           2       0.83      0.94      0.88      1982\n",
            "           3       0.99      0.85      0.92      2353\n",
            "           4       0.43      0.89      0.58       379\n",
            "           5       0.04      0.29      0.07        34\n",
            "           6       0.80      0.79      0.80       577\n",
            "           7       0.89      0.82      0.85       318\n",
            "           8       0.99      0.71      0.83       978\n",
            "           9       0.93      0.59      0.72       806\n",
            "\n",
            "    accuracy                           0.83      8630\n",
            "   macro avg       0.75      0.75      0.72      8630\n",
            "weighted avg       0.88      0.83      0.84      8630\n",
            "\n",
            "TEST  Accuracy: 0.8295 | Macro-F1: 0.7199\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.87      0.86       301\n",
            "           2       0.85      0.94      0.89       496\n",
            "           3       0.99      0.82      0.89       589\n",
            "           4       0.41      0.92      0.56        95\n",
            "           5       0.00      0.00      0.00         8\n",
            "           6       0.82      0.83      0.82       144\n",
            "           7       0.89      0.78      0.83        79\n",
            "           8       0.98      0.77      0.86       245\n",
            "           9       0.95      0.63      0.75       201\n",
            "\n",
            "    accuracy                           0.83      2158\n",
            "   macro avg       0.75      0.73      0.72      2158\n",
            "weighted avg       0.89      0.83      0.85      2158\n",
            "\n",
            "Train Error: 0.1694 | Test Error: 0.1705 | Gap: 0.0011\n",
            "\n",
            "[SUMMARY]\n",
            " {\n",
            "  \"embed\": {\n",
            "    \"type\": \"TruncatedSVD\",\n",
            "    \"dims\": 67,\n",
            "    \"l2\": true,\n",
            "    \"preprocessed_features\": 68,\n",
            "    \"variance_est\": 0.9999999999990805\n",
            "  },\n",
            "  \"k\": 36,\n",
            "  \"val_acc\": 0.7891077636152954,\n",
            "  \"train\": {\n",
            "    \"accuracy\": 0.8305909617612978,\n",
            "    \"macro_f1\": 0.7238524449147126,\n",
            "    \"error\": 0.16940903823870224\n",
            "  },\n",
            "  \"test\": {\n",
            "    \"accuracy\": 0.8294717330861909,\n",
            "    \"macro_f1\": 0.719873669152478,\n",
            "    \"error\": 0.17052826691380907,\n",
            "    \"gap\": 0.0011192286751068314\n",
            "  }\n",
            "}\n",
            "\n",
            "Artifacts in: artifacts_model2_short\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np, matplotlib.pyplot as plt, os\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "k_grid = sorted(set([n_labels, 2*n_labels, 3*n_labels, 4*n_labels]))  # e.g., [9,18,27,36]\n",
        "train_errs, val_errs = [], []\n",
        "\n",
        "for k in k_grid:\n",
        "    mbk = MiniBatchKMeans(n_clusters=k, random_state=SEED, n_init=15, batch_size=4096, max_iter=250).fit(Ztr_tr)\n",
        "    mp = hungarian_map(mbk.labels_, ytr_tr)\n",
        "    y_tr_hat  = np.array([mp[c] for c in mbk.predict(Ztr_tr)], dtype=object)\n",
        "    y_val_hat = np.array([mp[c] for c in mbk.predict(Ztr_val)], dtype=object)\n",
        "    train_errs.append(1.0 - accuracy_score(ytr_tr, y_tr_hat))\n",
        "    val_errs.append(1.0 - accuracy_score(ytr_val, y_val_hat))\n",
        "\n",
        "plt.figure(figsize=(6.5,4.5), dpi=140)\n",
        "plt.plot(k_grid, train_errs, marker=\"o\", label=\"Train error\")\n",
        "plt.plot(k_grid, val_errs, marker=\"o\", label=\"Validation error\")\n",
        "plt.title(\"Model 2: fitting graph (k vs. error)\")\n",
        "plt.xlabel(\"k (clusters)\"); plt.ylabel(\"Error = 1 - accuracy\"); plt.legend()\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, \"fitting_graph_k.png\"), dpi=160)\n",
        "plt.close()\n",
        "\n",
        "def save_cm_png(y_true, y_pred, title, fname):\n",
        "    labs = sorted(np.unique(np.concatenate([y_true, y_pred]).astype(str)))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labs)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labs)\n",
        "    fig, ax = plt.subplots(figsize=(6.2,5.4), dpi=140)\n",
        "    disp.plot(ax=ax, colorbar=False); ax.set_title(title); plt.xticks(rotation=30)\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(OUT_DIR, fname), dpi=160); plt.close()\n",
        "\n",
        "save_cm_png(ytr, ytr_pred, \"Model 2 — Confusion Matrix (TRAIN)\", \"cm_train.png\")\n",
        "save_cm_png(yte, yte_pred, \"Model 2 — Confusion Matrix (TEST)\",  \"cm_test.png\")\n",
        "\n",
        "print(\"Saved:\",\n",
        "      os.path.join(OUT_DIR, \"fitting_graph_k.png\"),\n",
        "      os.path.join(OUT_DIR, \"cm_train.png\"),\n",
        "      os.path.join(OUT_DIR, \"cm_test.png\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAEAztEXwsTi",
        "outputId": "a7520c8f-6399-4edc-fcc2-a3a3dedf2c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: artifacts_model2_short/fitting_graph_k.png artifacts_model2_short/cm_train.png artifacts_model2_short/cm_test.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIMH_1ZmvJj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}